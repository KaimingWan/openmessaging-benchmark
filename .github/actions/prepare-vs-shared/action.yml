name: "Execute Shared Steps"
inputs:
  streaming_provider:
    required: true
  automq_access_key:
    required: true
  automq_secret_key:
    required: true
  tf_backend_bucket:
    required: true
  tf_backend_key:
    required: true
  region:
    required: true
  cloud_provider:
    required: true
  ssh_private_key:
    required: true
  ssh_public_key:
    required: true
  infra_cost_api_key:
    required: true
  uninstall:
    required: true
  execute_benchmark:
    required: true
outputs:
  benchmark_result_json_automq:
    value: ${{ steps.output_benchmark_result.outputs.benchmark_result_json_automq }}
  extracted_data_automq:
    value: ${{ steps.extract_final_result.outputs.extracted_data_automq }}
  baseline_cost_automq:
    value: ${{ steps.extract_costs.outputs.baseline_cost_automq }}
  usage_cost_automq:
    value: ${{ steps.extract_costs.outputs.usage_cost_automq }}
  total_cost_automq:
    value: ${{ steps.extract_costs.outputs.total_cost_automq }}
  benchmark_result_json_kafka:
    value: ${{ steps.output_benchmark_result.outputs.benchmark_result_json_kafka }}
  extracted_data_kafka:
    value: ${{ steps.extract_final_result.outputs.extracted_data_kafka }}
  baseline_cost_kafka:
    value: ${{ steps.extract_costs.outputs.baseline_cost_kafka }}
  usage_cost_kafka:
    value: ${{ steps.extract_costs.outputs.usage_cost_kafka }}
  total_cost_kafka:
    value: ${{ steps.extract_costs.outputs.total_cost_kafka }}

runs:
  using: "composite"
  steps:
    ## reference: https://www.jameskerr.blog/posts/sharing-steps-in-github-action-workflows/

    - name: Fetch Latest Data
      shell: bash
      run: |
        git pull origin main

    - name: Build Benchmark Code
      shell: bash
      run: |
        mvn clean package -Dlicense.skip=true -Dcheckstyle.skip -DskipTests -Dspotless.check.skip

    - name: Apply Variables and Secrets for Shared Files
      shell: bash
      run: |
        echo "current path is: $(pwd)"
        sed -i "s/\${AUTOMQ_ENVID}/${{ inputs.streaming_provider }}/g" "driver-nats/deploy/provision-nats-aws.tf"
        sed -i "s/\${AUTOMQ_ENVID}/${{ inputs.streaming_provider }}/g" "driver-pravega/deploy/provision-pravega-aws.tf"
        sed -i "s/\${AUTOMQ_ENVID}/${{ inputs.streaming_provider }}/g" "driver-${{ inputs.streaming_provider }}/deploy/${{ inputs.cloud_provider }}/provision-kafka-aws.tf"

    - name: Apply Variables and Secrets for Streaming Provider
      working-directory: driver-${{ inputs.streaming_provider }}/deploy/${{ inputs.cloud_provider }}
      ## Set AK/SK and terraform s3 backend info
      shell: bash
      run: |
        echo "current path is: $(pwd)"
        sed -i "s/\${TF_BACKEND_BUCKET}/$TF_BACKEND_BUCKET/g" "provision-kafka-aws.tf"
        sed -i "s/\${TF_BACKEND_KEY}/$TF_BACKEND_KEY/g" "provision-kafka-aws.tf"
        sed -i "s/\${TF_BACKEND_REGION}/$REGION/g" "provision-kafka-aws.tf"
        sed -i "s/\${AUTOMQ_ACCESS_KEY}/${{ inputs.automq_access_key }}/g" "var.tfvars"
        sed -i "s/\${AUTOMQ_SECRET_KEY}/${{ inputs.automq_secret_key }}/g" "var.tfvars"
      env:
        ## sed match only support ENV rather than expression like ${{ secrets.TF_BACKEND_BUCKET }}
        TF_BACKEND_BUCKET: ${{ inputs.tf_backend_bucket }}
        TF_BACKEND_KEY: ${{ inputs.tf_backend_key }}-${{ inputs.cloud_provider }}-${{ inputs.streaming_provider }}
        REGION: ${{ inputs.region }}


    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ inputs.automq_access_key }}
        aws-secret-access-key: ${{ inputs.automq_secret_key }}
        aws-region: ${{ inputs.region }}

    - name: Setup SSH key
      shell: bash
      run: |
        mkdir -p ~/.ssh
        echo "${{ inputs.ssh_private_key }}" > ~/.ssh/${{ inputs.streaming_provider }}_${{ inputs.cloud_provider }}
        echo "${{ inputs.ssh_public_key }}" > ~/.ssh/${{ inputs.streaming_provider }}_${{ inputs.cloud_provider }}.pub
        chmod 600 ~/.ssh/${{ inputs.streaming_provider }}_${{ inputs.cloud_provider }}
        chmod 644 ~/.ssh/${{ inputs.streaming_provider }}_${{ inputs.cloud_provider }}.pub

    - name: Install python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Setup Infra Cost
      shell: bash
      run: |
        # Downloads the CLI based on your OS/arch and puts it in /usr/local/bin
        curl -fsSL https://raw.githubusercontent.com/infracost/infracost/master/scripts/install.sh | sh
        infracost --version
        infracost configure set api_key ${{ inputs.infra_cost_api_key }}

    - name: AWS Cost Estimate
      shell: bash
      run: |
        echo "[INFO] Provider is AutoMQ"
        cd  driver-${{ inputs.streaming_provider }}/deploy/${{ inputs.cloud_provider }}
        infracost breakdown --path . --usage-file infracost/${{ inputs.cloud_provider }}-medium-500m-6t.yml >> /tmp/aws-cost.txt
        
        COST_DETAIL_FILE=/tmp/aws-cost.txt
        cat $COST_DETAIL_FILE

    - name: Read and extract costs from file
      id: extract_costs
      shell: bash
      run: |
        python3 workflow_scripts/python/extract_cost_info.py
      env:
        STREAMING_PROVIDER: ${{ inputs.streaming_provider }}

    - name: Output Costs
      shell: bash
      run: |
        if [ "${{ inputs.streaming_provider }}" = "automq" ]; then
          echo "Baseline cost: ${{ steps.extract_costs.outputs.baseline_cost_automq }}"
          echo "Usage cost: ${{ steps.extract_costs.outputs.usage_cost_automq }}"
          echo "Total cost: ${{ steps.extract_costs.outputs.total_cost_automq }}"
        elif [ "${{ inputs.streaming_provider }}" = "kafka" ]; then
          echo "Baseline cost: ${{ steps.extract_costs.outputs.baseline_cost_kafka }}"
          echo "Usage cost: ${{ steps.extract_costs.outputs.usage_cost_kafka }}"
          echo "Total cost: ${{ steps.extract_costs.outputs.total_cost_kafka }}"
        fi

    - name: Setup terraform
      uses: hashicorp/setup-terraform@v3

    - name: Initialize terraform
      working-directory: driver-${{ inputs.streaming_provider }}/deploy/${{ inputs.cloud_provider }}
      shell: bash
      run: terraform init

    - name: Uninstall Cloud Infra
      working-directory: driver-${{ inputs.streaming_provider }}/deploy/${{ inputs.cloud_provider }}
      shell: bash
      run: |
        if [ "${{ inputs.uninstall }}" == "true" ]; then
          terraform destroy --auto-approve -var-file var.tfvars
        fi     


    - name: Terraform Plan
      working-directory: driver-${{ inputs.streaming_provider }}/deploy/${{ inputs.cloud_provider }}
      shell: bash
      run: terraform plan -var-file var.tfvars


    - name: Apply terraform
      working-directory: driver-${{ inputs.streaming_provider }}/deploy/${{ inputs.cloud_provider }}
      shell: bash
      run: |
        if [ "${{ inputs.uninstall }}" = "false" ]; then
          terraform apply --auto-approve -var-file var.tfvars
        fi


    - name: Install ansible
      shell: bash
      run: |
        if [[ "${{ inputs.uninstall }}" = "false" && "${{ inputs.execute_benchmark }}" = "false" ]]; then
          python -m pip install --upgrade pip
          python -m pip install --user ansible
          python -m pip install --user jmespath
        fi


    - name: Download Latest AutoMQ TGZ File
      shell: bash
      run: |
        if [[ "${{ inputs.uninstall }}" = "false" && "${{ inputs.execute_benchmark }}" = "false" && "${{ inputs.streaming_provider }}" = "automq" ]]; then
          curl -L https://download.automq.com/community_edition/artifacts/automq-kafka-latest.tgz -o /tmp/automq-kafka-latest.tgz
        fi


    - name: Install Streaming Cluster
      working-directory: driver-${{ inputs.streaming_provider }}/deploy/${{ inputs.cloud_provider }}
      shell: bash
      run: |
        if [[ "${{ inputs.uninstall }}" = "false" && "${{ inputs.execute_benchmark }}" = "false" ]]; then
          if [ "${{ inputs.streaming_provider }}" = "automq" ]; then
              echo "Install AutoMQ"
              ansible-playbook deploy.yaml -i hosts.ini
          elif [ "${{ inputs.streaming_provider }}" = "kafka" ]; then
              echo "Install Apache Kafka"
              wget https://github.com/adammck/terraform-inventory/releases/download/v0.10/terraform-inventory_v0.10_linux_amd64.zip
              unzip terraform-inventory_v0.10_linux_amd64.zip
              mv terraform-inventory /usr/local/bin
              ansible-playbook   --user ubuntu --inventory `which terraform-inventory` deploy.yaml 
          fi
        fi
      

    - name: Execute Benchmark
      working-directory: driver-${{ inputs.streaming_provider }}/deploy/${{ inputs.cloud_provider }}
      shell: bash
      run: |
        if [ "${{ inputs.execute_benchmark }}" = "true" ]; then
          chmod +x $GITHUB_WORKSPACE/workflow_scripts/bin/execute_benchmark.sh
          sh $GITHUB_WORKSPACE/workflow_scripts/bin/execute_benchmark.sh       
        fi
      env:
        STREAMING_PROVIDER: ${{ inputs.streaming_provider }}
        CLOUD_PROVIDER: ${{ inputs.cloud_provider }}



    - name: Output Benchmark Result
      shell: bash
      id: output_benchmark_result
      run: |
        if [ "${{ inputs.execute_benchmark }}" = "true" ]; then
          sudo apt-get install jq
          JSON_FILE=$(ls /tmp/*.json | head -n 1)
          extracted_benchmark_output=$(cat $JSON_FILE)
          echo "benchmark_result_json_$STREAMING_PROVIDER<<EOF" >> $GITHUB_OUTPUT
          echo "$extracted_benchmark_output" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
        fi
      env:
        STREAMING_PROVIDER: ${{ inputs.streaming_provider }}

    - name: Install python dependencies
      shell: bash
      run: |
        if [ "${{ inputs.execute_benchmark }}" = "true" ]; then
          python -m pip install --upgrade pip
          pip install regex 
        fi


    - name: Extract Information from Benchmark Worker Log
      shell: bash
      id: extract_final_result
      run: |
        if [ "${{ inputs.execute_benchmark }}" = "true" ]; then
          python3 workflow_scripts/python/extract_info_from_benchmark.py
          extracted_data=$(cat /tmp/extracted_data)
          echo "extracted_data_$STREAMING_PROVIDER<<EOF_MARKER" >> $GITHUB_OUTPUT
          echo "$extracted_data" >> $GITHUB_OUTPUT
          echo "EOF_MARKER" >> $GITHUB_OUTPUT
        fi
      env:
        STREAMING_PROVIDER: ${{ inputs.streaming_provider }}
