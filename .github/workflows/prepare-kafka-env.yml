name: Prepare Kafka Cluster

on:
  workflow_dispatch:
    inputs:
      cloud_provider:
        default: aws-cn
        required: true
        type: string
      region:
        default: cn-northwest-1
        required: true
        type: string
      uninstall:
        default: false
        required: true
        type: boolean
      execute_benchmark:
        default: false
        required: true
        type: boolean

jobs:
  prepare_env:
    name: Prepare AWS Environment
    runs-on: ubuntu-latest
    environment:  ${{ inputs.cloud_provider }}
    steps:
      - name: Checkout Benchmark Code
        uses: actions/checkout@v3

      - name: Cache local Maven repository
        uses: actions/cache@v2
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            ${{ runner.os }}-maven-

      - name: Fetch Latest Data
        run: |
          git pull origin main

      - name: Build Benchmark Code
        run: |
          mvn clean package -Dlicense.skip=true -Dcheckstyle.skip -DskipTests -Dspotless.check.skip

      - name: Apply Variables and Secrets for Shared Files
        run: |
          echo "current path is: $(pwd)"
          sed -i "s/\${AUTOMQ_ENVID}/kafka/g" "driver-nats/deploy/provision-nats-aws.tf"
          sed -i "s/\${AUTOMQ_ENVID}/kafka/g" "driver-pravega/deploy/provision-pravega-aws.tf"
          sed -i "s/\${AUTOMQ_ENVID}/kafka/g" "driver-kafka/deploy/ssd-kraft-deployment/provision-kafka-aws.tf"

      - name: Apply Variables and Secrets for Streaming Provider
        working-directory: driver-kafka/deploy/ssd-kraft-deployment
        ## Set AK/SK and terraform s3 backend info
        run: |
          echo "current path is: $(pwd)"
          sed -i "s/\${TF_BACKEND_BUCKET}/$TF_BACKEND_BUCKET/g" "$TF_FILENAME"
          TF_FILENAME=$TF_FILENAME
          sed -i "s/\${TF_BACKEND_KEY}/$TF_BACKEND_KEY/g" "$TF_FILENAME"
          sed -i "s/\${TF_BACKEND_REGION}/${{ inputs.region }}/g" "$TF_FILENAME"
          sed -i "s/\${AUTOMQ_ACCESS_KEY}/${{ secrets.AUTOMQ_ACCESS_KEY }}/g" "$TF_VAR_FILENAME"
          sed -i "s/\${AUTOMQ_SECRET_KEY}/${{ secrets.AUTOMQ_SECRET_KEY }}/g" "$TF_VAR_FILENAME"
        env:
          TF_BACKEND_BUCKET: ${{ secrets.TF_BACKEND_BUCKET }}
          TF_BACKEND_KEY: ${{ secrets.TF_BACKEND_KEY }}-kafka
          TF_FILENAME: provision-kafka-aws.tf
          TF_VAR_FILENAME: terraform.tfvars

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AUTOMQ_ACCESS_KEY }}
          aws-secret-access-key: ${{ secrets.AUTOMQ_SECRET_KEY }}
          aws-region: ${{ inputs.region }}

      - name: Setup SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/automq_aws
          echo "${{ secrets.SSH_PUBLIC_KEY }}" > ~/.ssh/automq_aws.pub
          chmod 600 ~/.ssh/automq_aws
          chmod 644 ~/.ssh/automq_aws.pub

      - name: Install python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Setup Infra Cost
        run: |
          # Downloads the CLI based on your OS/arch and puts it in /usr/local/bin
          curl -fsSL https://raw.githubusercontent.com/infracost/infracost/master/scripts/install.sh | sh
          infracost --version
          infracost configure set api_key ${{ secrets.INFRA_COST_API_KEY }}

      - name: AWS Cost Estimate
        run: |
          echo "[INFO] Provider is AutoMQ"
          cd  driver-kafka/deploy/ssd-kraft-deployment
          infracost breakdown --path . --usage-file infracost/automq-medium-s3-500m-6t.yml >> /tmp/aws-cost.txt
          
          COST_DETAIL_FILE=/tmp/aws-cost.txt
          cat $COST_DETAIL_FILE

      - name: Read and extract costs from file
        id: extract_costs
        run: |
          python3 - <<'EOF'
          import re
          import os
          import sys
          
          with open('/tmp/aws-cost.txt', 'r') as file:
              output = file.read()
          
          print("File content:")
          print(output)
          
          pattern = re.compile(r'â”ƒ\s*main\s*â”ƒ\s*\$(\d+)\s*â”ƒ\s*\$(\d+)\s*â”ƒ\s*\$(\d+)\s*â”ƒ')
          match = pattern.search(output)
          
          print(f"Match: {match}")
          
          if match:
              baseline_cost = match.group(1)
              usage_cost = match.group(2)
              total_cost = match.group(3)
          
              print(f"Baseline cost: ${baseline_cost}")
              print(f"Usage cost: ${usage_cost}")
              print(f"Total cost: ${total_cost}")
          
              github_output = os.getenv('GITHUB_OUTPUT', 'output.txt')
              with open(github_output, 'a') as output_file:
                  output_file.write(f'baseline_cost={baseline_cost}\n')
                  output_file.write(f'usage_cost={usage_cost}\n')
                  output_file.write(f'total_cost={total_cost}\n')
          else:
              print("Can't extract cost info")
              sys.exit(1)  # è§¦å‘é”™è¯¯å¹¶é€€å‡º
          EOF


      - name: Output Costs
        run: |
          echo "Baseline cost: ${{ steps.extract_costs.outputs.baseline_cost }}"
          echo "Usage cost: ${{ steps.extract_costs.outputs.usage_cost }}"
          echo "Total cost: ${{ steps.extract_costs.outputs.total_cost }}"

      - name: Setup terraform
        uses: hashicorp/setup-terraform@v3

      - name: Initialize terraform
        working-directory: driver-kafka/deploy/ssd-kraft-deployment
        run: terraform init

      - name: Uninstall Cloud Infra
        working-directory: driver-kafka/deploy/ssd-kraft-deployment
        if: ${{ inputs.uninstall }}
        run: terraform destroy --auto-approve -var-file terraform.tfvars


      - name: Terraform Plan
        working-directory: driver-kafka/deploy/ssd-kraft-deployment
        run: terraform plan -var-file terraform.tfvars


      - name: Apply terraform
        working-directory: driver-kafka/deploy/ssd-kraft-deployment
        if: ${{ !inputs.uninstall }}
        run: terraform apply --auto-approve -var-file terraform.tfvars

      - name: Install ansible
        if: ${{ !inputs.uninstall && !inputs.execute_benchmark }}
        run: |
          python -m pip install --upgrade pip
          python -m pip install --user ansible
          python -m pip install --user jmespath

      - name: Download Latest AutoMQ TGZ File
        if: ${{ !inputs.uninstall && !inputs.execute_benchmark &&  inputs.streaming_provider == 'automq' }}
        run: |
          curl -L https://download.automq.com/community_edition/artifacts/automq-kafka-latest.tgz -o /tmp/automq-kafka-latest.tgz

      - name: Install AutoMQ Cluster
        working-directory: driver-automq/deploy
        ##todo support other streaming provider later
        if: ${{ !inputs.uninstall && !inputs.execute_benchmark && inputs.streaming_provider == 'automq' }}
        run: |
          ansible-playbook deploy.yaml -i terraform-aws/hosts.ini
          

      - name: Execute Benchmark
        working-directory: driver-kafka/deploy/ssd-kraft-deployment
        if: ${{ inputs.execute_benchmark }}
        run: |
          # Set up base SSH and SCP commands with common options
          SSH_BASE_CMD="ssh -o StrictHostKeyChecking=no -i ~/.ssh/automq_aws"
          SCP_BASE_CMD="scp -o StrictHostKeyChecking=no -i ~/.ssh/automq_aws"
          SSH_HOST="$(terraform output --raw user)@$(terraform output --raw client_ssh_host)"
          BENCHMARK_DIR="/opt/benchmark"
          
          # Delete old benchmark result files
          sudo rm -f /tmp/*.json
          $SSH_BASE_CMD $SSH_HOST "sudo rm -f $BENCHMARK_DIR/*.json"
          $SSH_BASE_CMD $SSH_HOST "sudo rm -f $BENCHMARK_DIR/benchmark-worker.log"
          
          # Execute the benchmark test
          $SSH_BASE_CMD $SSH_HOST "cd $BENCHMARK_DIR && sudo ./bin/benchmark -d ./driver-automq/driver.yaml ./driver-automq/workloads/fast-tail-read-500m.yaml"
          
          # Check if new result files have been generated
          TIMEOUT=7200  # 2-hour timeout
          ELAPSED=0
          CHECK_INTERVAL=5  # Check every 5 seconds
          
          while [ $ELAPSED -lt $TIMEOUT ]; do
            if $SSH_BASE_CMD $SSH_HOST "ls $BENCHMARK_DIR/*.json"; then
              echo "Benchmark results are ready."
              break
            else
              echo "Waiting for benchmark results..."
              sleep $CHECK_INTERVAL
              ELAPSED=$(($ELAPSED + $CHECK_INTERVAL))
            fi
          done
          
          if [ $ELAPSED -lt $TIMEOUT ]; then
            # Copy the result files to local directory when they exist
            $SCP_BASE_CMD $SSH_HOST:$BENCHMARK_DIR/*.json /tmp
            $SCP_BASE_CMD $SSH_HOST:$BENCHMARK_DIR/benchmark-worker.log /tmp
          else
            # Exit with an error message if the timeout is reached without results
            echo "Timeout waiting for benchmark results."
            exit 1
          fi


      - name: Output Benchmark Result
        if: ${{ inputs.execute_benchmark }}
        run: |
          sudo apt-get install jq
          JSON_FILE=$(ls /tmp/*.json | head -n 1)
          cat $JSON_FILE

      - name: Install python dependencies
        if: ${{ inputs.execute_benchmark }}
        run: |
            python -m pip install --upgrade pip
            pip install regex

      - name: Extract Information from Benchmark Worker Log
        if: ${{ inputs.execute_benchmark }}
        run: |
          python3 - <<'EOF'
          import re
          import json
          
          log_file_path = "/tmp/benchmark-worker.log"
          output_file_path = "/tmp/extracted_data"
          
          # Define regex patterns
          kafka_benchmark_driver_pattern = r'\[.*?\] INFO Benchmark - Initialized Kafka benchmark driver with common config: \{(.*?)\}, producer config: \{(.*?)\}, consumer config: \{(.*?)\}, topic config: \{(.*?)\}, replicationFactor: (\d+)'
          workloads_pattern = r'Workloads: \{(.*?)\}'
          
          # Read log file
          with open(log_file_path, 'r') as file:
              log_content = file.read()
          
          workloads_match = re.search(workloads_pattern, log_content, re.DOTALL)
          if workloads_match:
              workloads_str = workloads_match.group(1).strip()
          else:
              workloads_str = "Not found"
          
          def extract_workload_config(config):
              def get_value(key):
                  start_index = config.find(f'"{key}"') + len(key) + 4
                  end_index = config.find(',', start_index)
                  if end_index == -1:
                      end_index = config.find('\n', start_index)
                  value = config[start_index:end_index].strip()
                  if value.endswith(','):
                      value = value[:-1]
                  return value.replace('"', '')
          
              keys = [
                  "name", "topics", "partitionsPerTopic", "partitionsPerTopicList", "randomTopicNames",
                  "keyDistributor", "messageSize", "useRandomizedPayloads", "randomBytesRatio",
                  "randomizedPayloadPoolSize", "payloadFile", "subscriptionsPerTopic", "producersPerTopic",
                  "producersPerTopicList", "consumerPerSubscription", "producerRate", "producerRateList",
                  "consumerBacklogSizeGB", "backlogDrainRatio", "testDurationMinutes", "warmupDurationMinutes",
                  "logIntervalMillis"
              ]
          
              workload_dict = {}
              for key in keys:
                  value = get_value(key)
                  if value == 'null':
                      value = None
                  elif value == 'true':
                      value = True
                  elif value == 'false':
                      value = False
                  else:
                      try:
                          value = int(value)
                      except ValueError:
                          try:
                              value = float(value)
                          except ValueError:
                              pass
                  workload_dict[key] = value
          
              return workload_dict
          
          if workloads_str != "Not found":
              workloads = extract_workload_config(workloads_str)
          else:
              workloads = "Not found"
          
          # Extract KafkaBenchmarkDriver information
          kafka_benchmark_driver_match = re.search(kafka_benchmark_driver_pattern, log_content, re.DOTALL)
          if kafka_benchmark_driver_match:
              common_config = kafka_benchmark_driver_match.group(1).strip()
              producer_config = kafka_benchmark_driver_match.group(2).strip()
              consumer_config = kafka_benchmark_driver_match.group(3).strip()
              topic_config = kafka_benchmark_driver_match.group(4).strip()
              replicationFactor = kafka_benchmark_driver_match.group(5).strip()
          else:
              common_config = "Not found"
              producer_config = "Not found"
              consumer_config = "Not found"
              topic_config = "Not found"
              replicationFactor = "Not found"
          
          # Calculate average throughput
          throughput_pattern = r'WorkloadGenerator - Pub rate \d+\.\d+ msg/s \/ (\d+\.\d+) MB/s'
          throughput_matches = re.findall(throughput_pattern, log_content)
          average_throughput = sum(float(tp) for tp in throughput_matches) / len(throughput_matches) if throughput_matches else 0
          
          # Prepare data for output
          extracted_data = {
              "workload_config": workloads,
              "producer_config": producer_config,
              "consumer_config": consumer_config,
              "topic_config": topic_config,
              "replication_factor": replicationFactor,
              "average_throughput": average_throughput
          }
          
          # Write to output file
          with open(output_file_path, 'w') as outfile:
              json.dump(extracted_data, outfile, indent=4)
          
          # Print the extracted data for verification
          print(json.dumps(extracted_data, indent=4))
          EOF
          

      - name: Generate Report in Issue
        uses: actions/github-script@v6
        if: ${{ inputs.execute_benchmark }}
        env:
          JSON_FILE_PATH: /tmp
          EXTRACTED_DATA_PATH: /tmp/extracted_data
          BASELINE_COST: ${{ steps.extract_costs.outputs.baseline_cost }}
          USAGE_COST: ${{ steps.extract_costs.outputs.usage_cost }}
          TOTAL_COST: ${{ steps.extract_costs.outputs.total_cost }}
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const path = require('path');
            
            const issue_number = 2;  // Specify the issue number
            const directoryPath = process.env.JSON_FILE_PATH;
            const extractedDataPath = process.env.EXTRACTED_DATA_PATH;
            
            // read cost report
            const costReportPath = fs.readFileSync('/tmp/aws-cost.txt', 'utf8');
            
            // Read the directory and find the first JSON file
            const files = fs.readdirSync(directoryPath).filter(file => file.endsWith('.json'));
            if (files.length === 0) {
              console.log("No JSON file found.");
              return;
            }
            const firstJsonFile = files[0];
            const filePath = path.join(directoryPath, firstJsonFile);
            
            // Read the content of the JSON file
            const jsonData = JSON.parse(fs.readFileSync(filePath, 'utf8'));

            // Extract specific fields
            const workload = jsonData.workload;
            const messageSize = jsonData.messageSize;
            const topics = jsonData.topics;
            const partitions = jsonData.partitions;
            const producersPerTopic = jsonData.producersPerTopic;
            const consumersPerTopic = jsonData.consumersPerTopic;
            
            // Extract latency metrics
            const latencyAvg = jsonData.aggregatedEndToEndLatencyAvg;
            const latency95pct = jsonData.aggregatedEndToEndLatency95pct;
            const latency99pct = jsonData.aggregatedEndToEndLatency99pct;
            const latency999pct = jsonData.aggregatedEndToEndLatency999pct;
            
            
            //process extracted data
            let extractedData = JSON.parse(fs.readFileSync(extractedDataPath, 'utf8'));
            
            // Extract specific fields from benchmark log json and extracted data
            const {
              workload_config,
              producer_config,
              consumer_config,
              topic_config,
              replication_factor,
              average_throughput
            } = extractedData;
            
            const extractWorkloadConfig = (config) => {
            const keys = [
              'name', 'topics', 'partitionsPerTopic', 'partitionsPerTopicList', 'randomTopicNames',
              'keyDistributor', 'messageSize', 'useRandomizedPayloads', 'randomBytesRatio',
              'randomizedPayloadPoolSize', 'payloadFile', 'subscriptionsPerTopic', 'producersPerTopic',
              'producersPerTopicList', 'consumerPerSubscription', 'producerRate', 'producerRateList',
              'consumerBacklogSizeGB', 'backlogDrainRatio', 'testDurationMinutes', 'warmupDurationMinutes',
              'logIntervalMillis'
            ];
          
            const pairs = keys.map(key => `${key}: ${config[key]}`);
              return pairs.join('\n');
            };
            
            const workloadConfigPairs = extractWorkloadConfig(workload_config);
            const configToKeyValuePairs = (config) => {
              return config.split(', ').map(pair => pair.replace('=', ': ')).join('\n');
            };
            
            const producerConfigPairs = configToKeyValuePairs(producer_config);
            const consumerConfigPairs = configToKeyValuePairs(consumer_config);
            const topicConfigPairs = configToKeyValuePairs(topic_config);
            const replicationFactorPairs = configToKeyValuePairs(replication_factor);
            
            // Costs are directly used from the steps
            const baselineCost = process.env.BASELINE_COST;
            const usageCost = process.env.USAGE_COST;
            const totalCost = process.env.TOTAL_COST;
            
            // remove fixed client cost involved by infracost
            function extractAndSubtract(value) {
              const match = value.match(/\$([0-9,.]+)/);
                if (match && match[1]) {
                const number = parseFloat(match[1].replace(/,/g, ''));
                const result = number - 286.34;
                return `$${result.toFixed(2)}`;
              } else {
                throw new Error("Invalid value format");
              }
            }
            
            // Get current date and time
            const now = new Date();
            const currentDate = now.toISOString().split('T')[0];
            const currentTime = now.toTimeString().split(' ')[0];
            
            // Generate a Markdown formatted report
            const markdownReport = `
              ## AutoMQ Benchmark VS. Result ðŸš€
              ### Benchmark Info
              **Report Generated:** ${currentDate} ${currentTime}
              ### Workload Configuration
              ${workloadConfigPairs}
              ### Producer Configuration
              ${producerConfigPairs}
              ### Consumer Configuration
              ${consumerConfigPairs}
              ### Topic Configuration
              ${topicConfigPairs}
              replicationFactor: ${replicationFactorPairs}
              ### Replication Configuration
              Average Throughput: ${average_throughput} MB/s
            
              > Cost Estimate Rule: AutoMQ 800MB of storage corresponds to about 25 PUTs and 10 GETs.We have estimated that each GB corresponds to 31.25 PUTs and 12.5 GETs.Assuming a peak throughput of 0.5 GB/s and an average throughput of 0.01 GB/s, with data retention for 7 days, the data volume for 30 days(Assume there always have 7 days historical data) is:7x24x3600x0.01GB/s = 6048GB = 5.9T â‰ˆ 6T


              | Streaming System | E2E LatencyAvg(ms) | E2E P95 Latency(ms) | E2E P99 Latency(ms) | Baseline Cost | Usage Cost | Total Cost |
              | ---------------- | ------------------ | ------------------- | ------------------- | ------------- | ---------- | ---------- |
              | Apache Kafka           | ${latencyAvg}      | ${latency95pct}     | ${latency99pct}     | $${baselineCost} | $${usageCost} | $${totalCost} |
            `;
      
            // Post the report as a comment to the specified issue
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: issue_number,
              body: markdownReport
            });
