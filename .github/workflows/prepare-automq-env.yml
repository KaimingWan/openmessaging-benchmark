name: Prepare AutoMQ Cluster

on:
  workflow_dispatch:
    inputs:
      cloud_provider:
        default: aws-cn
        required: true
        type: string
      streaming_provider:
        default: automq
        required: true
        type: string
      region:
        default: cn-northwest-1
        required: true
        type: string
      uninstall:
        default: false
        required: true
        type: boolean
      execute_benchmark:
        default: false
        required: true
        type: boolean

jobs:
  prepare_env:
    name: Prepare AWS Environment
    runs-on: ubuntu-latest
    environment:  ${{ inputs.cloud_provider }}
    steps:
      - name: Checkout Benchmark Code
        uses: actions/checkout@v3

      - name: Cache local Maven repository
        uses: actions/cache@v2
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            ${{ runner.os }}-maven-  

      - name: Build Benchmark Code
        run: |
          mvn clean package -Dlicense.skip=true -Dcheckstyle.skip -DskipTests -Dspotless.check.skip

      - name: Apply Variables and Secrets for Shared Files
        run: |
          echo "current path is: $(pwd)"
          sed -i "s/\${AUTOMQ_ENVID}/${{ inputs.streaming_provider }}/g" "driver-nats/deploy/provision-nats-aws.tf"
          sed -i "s/\${AUTOMQ_ENVID}/${{ inputs.streaming_provider }}/g" "driver-pravega/deploy/provision-pravega-aws.tf"
          sed -i "s/\${AUTOMQ_ENVID}/${{ inputs.streaming_provider }}/g" "driver-automq/deploy/terraform-aws/provision-kafka-aws.tf"

      - name: Apply Variables and Secrets for Streaming Provider
        working-directory: driver-automq/deploy/terraform-aws
        ## Set AK/SK and terraform s3 backend info
        run: |
          echo "current path is: $(pwd)"
          sed -i "s/\${TF_BACKEND_BUCKET}/$TF_BACKEND_BUCKET/g" "$TF_FILENAME"
          TF_FILENAME=$TF_FILENAME
          sed -i "s/\${TF_BACKEND_KEY}/$TF_BACKEND_KEY/g" "$TF_FILENAME"
          sed -i "s/\${TF_BACKEND_REGION}/${{ inputs.region }}/g" "$TF_FILENAME"
          sed -i "s/\${AUTOMQ_ACCESS_KEY}/${{ secrets.AUTOMQ_ACCESS_KEY }}/g" "$TF_VAR_FILENAME"
          sed -i "s/\${AUTOMQ_SECRET_KEY}/${{ secrets.AUTOMQ_SECRET_KEY }}/g" "$TF_VAR_FILENAME"
        env:
          TF_BACKEND_BUCKET: ${{ secrets.TF_BACKEND_BUCKET }}
          TF_BACKEND_KEY: ${{ secrets.TF_BACKEND_KEY }}-${{ inputs.streaming_provider }}
          TF_FILENAME: provision-kafka-aws.tf
          TF_VAR_FILENAME: terraform-aws-cn.tfvars

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AUTOMQ_ACCESS_KEY }}
          aws-secret-access-key: ${{ secrets.AUTOMQ_SECRET_KEY }}
          aws-region: ${{ inputs.region }}

      - name: Setup SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/automq_aws
          echo "${{ secrets.SSH_PUBLIC_KEY }}" > ~/.ssh/automq_aws.pub
          chmod 600 ~/.ssh/automq_aws
          chmod 644 ~/.ssh/automq_aws.pub

      - name: Install python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Setup Infra Cost
        run: |
          # Downloads the CLI based on your OS/arch and puts it in /usr/local/bin
          curl -fsSL https://raw.githubusercontent.com/infracost/infracost/master/scripts/install.sh | sh
          infracost --version
          infracost configure set api_key ${{ secrets.INFRA_COST_API_KEY }}

      - name: AWS Cost Estimate
        run: |
          if [ "${{ inputs.streaming_provider }}" == "automq" ]; then
                echo "[INFO] Provider is AutoMQ"
                cd  driver-automq/deploy/terraform-aws
          fi
          infracost breakdown --path . >> /tmp/aws-cost.txt
          COST_DETAIL_FILE=/tmp/aws-cost.txt
          cat $COST_DETAIL_FILE

      - name: Read and extract costs from file
        id: extract_costs
        run: |
          import re
          
          with open('/tmp/aws-cost.txt', 'r') as file:
           output = file.read()
          
          baseline_cost_match = re.search(r'Baseline cost\s*â”ƒ\s*\$(\d+)', output)
          usage_cost_match = re.search(r'Usage cost\*\s*â”ƒ\s*\$(\d+)', output)
          total_cost_match = re.search(r'Total cost\s*â”ƒ\s*\$(\d+)', output)
          
          if baseline_cost_match and usage_cost_match and total_cost_match:
           baseline_cost = baseline_cost_match.group(1)
           usage_cost = usage_cost_match.group(1)
           total_cost = total_cost_match.group(1)
          
           print(f"Baseline cost: ${baseline_cost}")
           print(f"Usage cost: ${usage_cost}")
           print(f"Total cost: ${total_cost}")
          
           with open(os.getenv('GITHUB_OUTPUT'), 'a') as output_file:
               output_file.write(f'baseline_cost={baseline_cost}\n')
               output_file.write(f'usage_cost={usage_cost}\n')
               output_file.write(f'total_cost={total_cost}\n')
          else:
           print("Can't extract cost info")

      - name: Output Costs
        run: |
          echo "Baseline cost: ${{ steps.extract_costs.outputs.baseline_cost }}"
          echo "Usage cost: ${{ steps.extract_costs.outputs.usage_cost }}"
          echo "Total cost: ${{ steps.extract_costs.outputs.total_cost }}"

      - name: Setup terraform
        uses: hashicorp/setup-terraform@v3

      - name: Initialize terraform
        working-directory: driver-automq/deploy/terraform-aws
        run: terraform init

      - name: Uninstall Cloud Infra
        working-directory: driver-automq/deploy/terraform-aws
        if: ${{ inputs.uninstall }}
        run: terraform destroy --auto-approve -var-file terraform-aws-cn.tfvars


      - name: Terraform Plan
        working-directory: driver-automq/deploy/terraform-aws
        run: terraform plan -var-file terraform-aws-cn.tfvars


      - name: Apply terraform
        working-directory: driver-automq/deploy/terraform-aws
        if: ${{ !inputs.uninstall }}
        run: terraform apply --auto-approve -var-file terraform-aws-cn.tfvars

      - name: Install ansible
        if: ${{ !inputs.uninstall && !inputs.execute_benchmark }}
        run: |
          python -m pip install --upgrade pip
          python -m pip install --user ansible
          python -m pip install --user jmespath

      - name: Download Latest AutoMQ TGZ File
        if: ${{ !inputs.uninstall && !inputs.execute_benchmark &&  inputs.streaming_provider == 'automq' }}
        run: |
          curl -L https://download.automq.com/community_edition/artifacts/automq-kafka-latest.tgz -o /tmp/automq-kafka-latest.tgz

      - name: Install AutoMQ Cluster
        working-directory: driver-automq/deploy
        ##todo support other streaming provider later
        if: ${{ !inputs.uninstall && !inputs.execute_benchmark && inputs.streaming_provider == 'automq' }}
        run: |
          ansible-playbook deploy.yaml -i terraform-aws/hosts.ini
          

      - name: Execute Benchmark
        working-directory: driver-automq/deploy/terraform-aws
        if: ${{ inputs.execute_benchmark }}
        run: |
          # Set up base SSH and SCP commands with common options
          SSH_BASE_CMD="ssh -o StrictHostKeyChecking=no -i ~/.ssh/automq_aws"
          SCP_BASE_CMD="scp -o StrictHostKeyChecking=no -i ~/.ssh/automq_aws"
          SSH_HOST="$(terraform output --raw user)@$(terraform output --raw client_ssh_host)"
          BENCHMARK_DIR="/opt/benchmark"
          
          # Delete old benchmark result files
          sudo rm -f /tmp/*.json
          $SSH_BASE_CMD $SSH_HOST "sudo rm -f $BENCHMARK_DIR/*.json"
          
          # Execute the benchmark test
          $SSH_BASE_CMD $SSH_HOST "cd $BENCHMARK_DIR && sudo ./bin/benchmark -d ./driver-automq/driver.yaml ./driver-automq/workloads/fast-tail-read-500m.yaml"
          
          # Check if new result files have been generated
          TIMEOUT=7200  # 2-hour timeout
          ELAPSED=0
          CHECK_INTERVAL=5  # Check every 5 seconds
          
          while [ $ELAPSED -lt $TIMEOUT ]; do
            if $SSH_BASE_CMD $SSH_HOST "ls $BENCHMARK_DIR/*.json"; then
              echo "Benchmark results are ready."
              break
            else
              echo "Waiting for benchmark results..."
              sleep $CHECK_INTERVAL
              ELAPSED=$(($ELAPSED + $CHECK_INTERVAL))
            fi
          done
          
          if [ $ELAPSED -lt $TIMEOUT ]; then
            # Copy the result files to local directory when they exist
            $SCP_BASE_CMD $SSH_HOST:$BENCHMARK_DIR/*.json /tmp
          else
            # Exit with an error message if the timeout is reached without results
            echo "Timeout waiting for benchmark results."
            exit 1
          fi


      - name: Output Benchmark Result
        if: ${{ inputs.execute_benchmark }}
        run: |
          sudo apt-get install jq
          JSON_FILE=$(ls /tmp/*.json | head -n 1)
          cat $JSON_FILE

      - name: Generate Report in Issue
        uses: actions/github-script@v6
        if: ${{ inputs.execute_benchmark }}
        env:
          JSON_FILE_PATH: /tmp  # Define the path where JSON files are stored
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const path = require('path');
            
            const issue_number = 2;  // Specify the issue number
            const directoryPath = process.env.JSON_FILE_PATH;
            
            // Read the directory and find the first JSON file
            const files = fs.readdirSync(directoryPath).filter(file => file.endsWith('.json'));
            if (files.length === 0) {
              console.log("No JSON file found.");
              return;
            }
            const firstJsonFile = files[0];
            const filePath = path.join(directoryPath, firstJsonFile);
            
            // Read the content of the JSON file
            const jsonData = JSON.parse(fs.readFileSync(filePath, 'utf8'));
            
            // Extract specific fields
            const workload = jsonData.workload;
            const messageSize = jsonData.messageSize;
            const topics = jsonData.topics;
            const partitions = jsonData.partitions;
            const producersPerTopic = jsonData.producersPerTopic;
            const consumersPerTopic = jsonData.consumersPerTopic;
            
            // Extract latency metrics
            const latencyAvg = jsonData.aggregatedEndToEndLatencyAvg;
            const latency95pct = jsonData.aggregatedEndToEndLatency95pct;
            const latency99pct = jsonData.aggregatedEndToEndLatency99pct;
            
            // Get current date and time
            const now = new Date();
            const currentDate = now.toISOString().split('T')[0];
            const currentTime = now.toTimeString().split(' ')[0];
            
            // Generate a Markdown formatted report
            const markdownReport = `
              ## AutoMQ Benchmark VS. Result ðŸš€
              **Report Generated:** ${currentDate} ${currentTime}
              **Workload:** ${workload}
              **Message Size:** ${messageSize} bytes
              **Topics:** ${topics}
              **Partitions:** ${partitions}
              **Producers per Topic:** ${producersPerTopic}
              **Consumers per Topic:** ${consumersPerTopic}
            
              | Streaming System | E2E LatencyAvg(ms) | E2E P95 Latency(ms) | E2E P99 Latency(ms) | 
              | ---------------- | ------------------ | ------------------- | ------------------- |
              | AutoMQ           | ${latencyAvg}      | ${latency95pct}     | ${latency99pct}     | 
            
            `;
            
            // Post the report as a comment to the specified issue
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: issue_number,
              body: markdownReport
            });


