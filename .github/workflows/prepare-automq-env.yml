name: Prepare AutoMQ Cluster

on:
  workflow_dispatch:
    inputs:
      cloud_provider:
        default: aws-cn
        required: true
        type: string
      streaming_provider:
        default: automq
        required: true
        type: string
      region:
        default: cn-northwest-1
        required: true
        type: string
      uninstall:
        default: false
        required: true
        type: boolean
      execute_benchmark:
        default: false
        required: true
        type: boolean

jobs:
  prepare_env:
    name: Prepare AWS Environment
    runs-on: ubuntu-latest
    environment:  ${{ inputs.cloud_provider }}
    steps:
      - name: Checkout Benchmark Code
        uses: actions/checkout@v3

      - name: Cache local Maven repository
        uses: actions/cache@v2
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            ${{ runner.os }}-maven-  

      - name: Build Benchmark Code
        run: |
          mvn clean package -Dlicense.skip=true -Dcheckstyle.skip -DskipTests -Dspotless.check.skip

      - name: Apply Variables and Secrets for Shared Files
        run: |
          echo "current path is: $(pwd)"
          sed -i "s/\${AUTOMQ_ENVID}/${{ inputs.streaming_provider }}/g" "driver-nats/deploy/provision-nats-aws.tf"
          sed -i "s/\${AUTOMQ_ENVID}/${{ inputs.streaming_provider }}/g" "driver-pravega/deploy/provision-pravega-aws.tf"
          sed -i "s/\${AUTOMQ_ENVID}/${{ inputs.streaming_provider }}/g" "driver-automq/deploy/terraform-aws/provision-kafka-aws.tf"

      - name: Apply Variables and Secrets for Streaming Provider
        working-directory: driver-automq/deploy/terraform-aws
        ## Set AK/SK and terraform s3 backend info
        run: |
          echo "current path is: $(pwd)"
          sed -i "s/\${TF_BACKEND_BUCKET}/$TF_BACKEND_BUCKET/g" "$TF_FILENAME"
          TF_FILENAME=$TF_FILENAME
          sed -i "s/\${TF_BACKEND_KEY}/$TF_BACKEND_KEY/g" "$TF_FILENAME"
          sed -i "s/\${TF_BACKEND_REGION}/${{ inputs.region }}/g" "$TF_FILENAME"
          sed -i "s/\${AUTOMQ_ACCESS_KEY}/${{ secrets.AUTOMQ_ACCESS_KEY }}/g" "$TF_VAR_FILENAME"
          sed -i "s/\${AUTOMQ_SECRET_KEY}/${{ secrets.AUTOMQ_SECRET_KEY }}/g" "$TF_VAR_FILENAME"
        env:
          TF_BACKEND_BUCKET: ${{ secrets.TF_BACKEND_BUCKET }}
          TF_BACKEND_KEY: ${{ secrets.TF_BACKEND_KEY }}-${{ inputs.streaming_provider }}
          TF_FILENAME: provision-kafka-aws.tf
          TF_VAR_FILENAME: terraform-aws-cn.tfvars

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AUTOMQ_ACCESS_KEY }}
          aws-secret-access-key: ${{ secrets.AUTOMQ_SECRET_KEY }}
          aws-region: ${{ inputs.region }}

      - name: Setup SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/automq_aws
          echo "${{ secrets.SSH_PUBLIC_KEY }}" > ~/.ssh/automq_aws.pub
          chmod 600 ~/.ssh/automq_aws
          chmod 644 ~/.ssh/automq_aws.pub

      - name: Install python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Setup Infra Cost
        run: |
          # Downloads the CLI based on your OS/arch and puts it in /usr/local/bin
          curl -fsSL https://raw.githubusercontent.com/infracost/infracost/master/scripts/install.sh | sh
          infracost --version
          infracost configure set api_key ${{ secrets.INFRA_COST_API_KEY }}

      - name: AWS Cost Estimate
        run: |
          if [ "${{ inputs.streaming_provider }}" == "automq" ]; then
                echo "[INFO] Provider is AutoMQ"
                cd  driver-automq/deploy/terraform-aws
          fi
          infracost breakdown --path . >> /tmp/aws-cost.txt
          COST_DETAIL_FILE=/tmp/aws-cost.txt
          cat $COST_DETAIL_FILE

      - name: Read and extract costs from file
        id: extract_costs
        run: |
          python3 - <<'EOF'
          import re
          import os
          import sys
          
          with open('/tmp/aws-cost.txt', 'r') as file:
              output = file.read()
          
          print("File content:")
          print(output)
          
          pattern = re.compile(r'â”ƒ\s*main\s*â”ƒ\s*\$(\d+)\s*â”ƒ\s*\$(\d+)\s*â”ƒ\s*\$(\d+)\s*â”ƒ')
          match = pattern.search(output)
          
          print(f"Match: {match}")
          
          if match:
              baseline_cost = match.group(1)
              usage_cost = match.group(2)
              total_cost = match.group(3)
          
              print(f"Baseline cost: ${baseline_cost}")
              print(f"Usage cost: ${usage_cost}")
              print(f"Total cost: ${total_cost}")
          
              github_output = os.getenv('GITHUB_OUTPUT', 'output.txt')
              with open(github_output, 'a') as output_file:
                  output_file.write(f'baseline_cost={baseline_cost}\n')
                  output_file.write(f'usage_cost={usage_cost}\n')
                  output_file.write(f'total_cost={total_cost}\n')
          else:
              print("Can't extract cost info")
              sys.exit(1)  # è§¦å‘é”™è¯¯å¹¶é€€å‡º
          EOF


      - name: Output Costs
        run: |
          echo "Baseline cost: ${{ steps.extract_costs.outputs.baseline_cost }}"
          echo "Usage cost: ${{ steps.extract_costs.outputs.usage_cost }}"
          echo "Total cost: ${{ steps.extract_costs.outputs.total_cost }}"

      - name: Setup terraform
        uses: hashicorp/setup-terraform@v3

      - name: Initialize terraform
        working-directory: driver-automq/deploy/terraform-aws
        run: terraform init

      - name: Uninstall Cloud Infra
        working-directory: driver-automq/deploy/terraform-aws
        if: ${{ inputs.uninstall }}
        run: terraform destroy --auto-approve -var-file terraform-aws-cn.tfvars


      - name: Terraform Plan
        working-directory: driver-automq/deploy/terraform-aws
        run: terraform plan -var-file terraform-aws-cn.tfvars


      - name: Apply terraform
        working-directory: driver-automq/deploy/terraform-aws
        if: ${{ !inputs.uninstall }}
        run: terraform apply --auto-approve -var-file terraform-aws-cn.tfvars

      - name: Install ansible
        if: ${{ !inputs.uninstall && !inputs.execute_benchmark }}
        run: |
          python -m pip install --upgrade pip
          python -m pip install --user ansible
          python -m pip install --user jmespath

      - name: Download Latest AutoMQ TGZ File
        if: ${{ !inputs.uninstall && !inputs.execute_benchmark &&  inputs.streaming_provider == 'automq' }}
        run: |
          curl -L https://download.automq.com/community_edition/artifacts/automq-kafka-latest.tgz -o /tmp/automq-kafka-latest.tgz

      - name: Install AutoMQ Cluster
        working-directory: driver-automq/deploy
        ##todo support other streaming provider later
        if: ${{ !inputs.uninstall && !inputs.execute_benchmark && inputs.streaming_provider == 'automq' }}
        run: |
          ansible-playbook deploy.yaml -i terraform-aws/hosts.ini
          

      - name: Execute Benchmark
        working-directory: driver-automq/deploy/terraform-aws
        if: ${{ inputs.execute_benchmark }}
        run: |
          # Set up base SSH and SCP commands with common options
          SSH_BASE_CMD="ssh -o StrictHostKeyChecking=no -i ~/.ssh/automq_aws"
          SCP_BASE_CMD="scp -o StrictHostKeyChecking=no -i ~/.ssh/automq_aws"
          SSH_HOST="$(terraform output --raw user)@$(terraform output --raw client_ssh_host)"
          BENCHMARK_DIR="/opt/benchmark"
          
          # Delete old benchmark result files
          sudo rm -f /tmp/*.json
          $SSH_BASE_CMD $SSH_HOST "sudo rm -f $BENCHMARK_DIR/*.json"
          
          # Execute the benchmark test
          $SSH_BASE_CMD $SSH_HOST "cd $BENCHMARK_DIR && sudo ./bin/benchmark -d ./driver-automq/driver.yaml ./driver-automq/workloads/fast-tail-read-500m.yaml"
          
          # Check if new result files have been generated
          TIMEOUT=7200  # 2-hour timeout
          ELAPSED=0
          CHECK_INTERVAL=5  # Check every 5 seconds
          
          while [ $ELAPSED -lt $TIMEOUT ]; do
            if $SSH_BASE_CMD $SSH_HOST "ls $BENCHMARK_DIR/*.json"; then
              echo "Benchmark results are ready."
              break
            else
              echo "Waiting for benchmark results..."
              sleep $CHECK_INTERVAL
              ELAPSED=$(($ELAPSED + $CHECK_INTERVAL))
            fi
          done
          
          if [ $ELAPSED -lt $TIMEOUT ]; then
            # Copy the result files to local directory when they exist
            $SCP_BASE_CMD $SSH_HOST:$BENCHMARK_DIR/*.json /tmp
          else
            # Exit with an error message if the timeout is reached without results
            echo "Timeout waiting for benchmark results."
            exit 1
          fi


      - name: Output Benchmark Result
        if: ${{ inputs.execute_benchmark }}
        run: |
          sudo apt-get install jq
          JSON_FILE=$(ls /tmp/*.json | head -n 1)
          cat $JSON_FILE

      - name: Install python dependencies
        if: ${{ inputs.execute_benchmark }}
        run: |
            python -m pip install --upgrade pip
            pip install regex

      - name: Extract Information from Log
        if: ${{ inputs.execute_benchmark }}
        env:
          LOG_FILE_PATH: /opt/benchmark/benchmark-worker.log
        run: |
          python <<EOF
              import re
              import json
              
              log_file_path = "${{ env.LOG_FILE_PATH }}"
              output_file_path = "/tmp/extracted_data.json"
              
              # Define regex patterns
              workload_pattern = r'Workloads:\s*\{(.*?)\}'
              consumer_config_pattern = r'ConsumerConfig - ConsumerConfig values:\s*(.*?)\}'
              common_config_pattern = r'Initialized Kafka benchmark driver with common config:\s*\{(.*?)\}'
              throughput_pattern = r'WorkloadGenerator - Pub rate \d+\.\d+ msg/s \/ (\d+\.\d+) MB/s'
            
              # Read log file
              with open(log_file_path, 'r') as file:
              log_content = file.read()
            
              # Extract information
              workload_match = re.search(workload_pattern, log_content, re.DOTALL)
              consumer_config_match = re.search(consumer_config_pattern, log_content, re.DOTALL)
              common_config_match = re.search(common_config_pattern, log_content, re.DOTALL)
              throughput_matches = re.findall(throughput_pattern, log_content)
              
              # Calculate average throughput
              average_throughput = sum(float(tp) for tp in throughput_matches) / len(throughput_matches)
              
              # Prepare data for output
              extracted_data = {
                "workload": workload_match.group(1) if workload_match else "Not found",
                "consumer_config": consumer_config_match.group(1) if consumer_config_match else "Not found",
                "common_config": common_config_match.group(1) if common_config_match else "Not found",
                "average_throughput": average_throughput
              }
            
              # Write to output file
              with open(output_file_path, 'w') as outfile:
              json.dump(extracted_data, outfile)
          EOF

      - name: Upload Extracted Data
        if: ${{ inputs.execute_benchmark }}
        uses: actions/upload-artifact@v3
        with:
          name: extracted-data
          path: /tmp/extracted_data.json


      - name: Generate Report in Issue
        uses: actions/github-script@v6
        if: ${{ inputs.execute_benchmark }}
        env:
          JSON_FILE_PATH: /tmp
          EXTRACTED_DATA_PATH: /tmp/extracted_data.json
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const path = require('path');
            
            const issue_number = 2;  // Specify the issue number
            const directoryPath = process.env.JSON_FILE_PATH;
            const extractedDataPath = process.env.EXTRACTED_DATA_PATH;
            
            // Read the directory and find the first JSON file
            const files = fs.readdirSync(directoryPath).filter(file => file.endsWith('.json'));
            if (files.length === 0) {
              console.log("No JSON file found.");
              return;
            }
            const firstJsonFile = files[0];
            const filePath = path.join(directoryPath, firstJsonFile);
            
            // Read the content of the JSON file
            const jsonData = JSON.parse(fs.readFileSync(filePath, 'utf8'));
            
            // Read extracted data
            const extractedData = JSON.parse(fs.readFileSync(extractedDataPath, 'utf8'));
            
            // Extract specific fields from JSON and extracted data
            const {
              workload,
              messageSize,
              topics,
              partitions,
              producersPerTopic,
              consumersPerTopic,
              latencyAvg,
              latency95pct,
              latency99pct
            } = jsonData;
            const {
              workloadConfig,
              consumerConfig,
              commonConfig,
              averageThroughput
            } = extractedData;
            
            // Costs are directly used from the steps
            const baselineCost = '${{ steps.extract_costs.outputs.baseline_cost }}';
            const usageCost = '${{ steps.extract_costs.outputs.usage_cost }}';
            const totalCost = '${{ steps.extract_costs.outputs.total_cost }}';
            
            // Get current date and time
            const now = new Date();
            const currentDate = now.toISOString().split('T')[0];
            const currentTime = now.toTimeString().split(' ')[0];
            
            // Generate a Markdown formatted report
            const markdownReport = `
              ## AutoMQ Benchmark VS. Result ðŸš€
              **Report Generated:** ${currentDate} ${currentTime}
              **Workload:** ${workload}
              **Message Size:** ${messageSize} bytes
              **Topics:** ${topics}
              **Partitions:** ${partitions}
              **Producers per Topic:** ${producersPerTopic}
              **Consumers per Topic:** ${consumersPerTopic}
            
              **Configurations and Throughput:**
              **Workload Configuration:**
              \`\`\`${workloadConfig}\`\`\`
              **Consumer Configuration:**
              \`\`\`${consumerConfig}\`\`\`
              **Common Configuration:**
              \`\`\`${commonConfig}\`\`\`
              **Average Throughput:** ${averageThroughput} MB/s
            
              | Streaming System | E2E LatencyAvg(ms) | E2E P95 Latency(ms) | E2E P99 Latency(ms) | Baseline Cost | Usage Cost | Total Cost |
              | ---------------- | ------------------ | ------------------- | ------------------- | ------------ | ---------- | ---------- |
              | AutoMQ           | ${latencyAvg}      | ${latency95pct}     | ${latency99pct}     | \$${baselineCost} | \$${usageCost} | \$${totalCost} |
            `;
            
            // Post the report as a comment to the specified issue
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: issue_number,
              body: markdownReport
            });